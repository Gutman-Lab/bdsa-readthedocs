{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    SegformerForSemanticSegmentation, \n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from dsa_helpers.ml.transforms.segformer_transforms import (\n",
    "    train_transforms, val_transforms\n",
    ")\n",
    "from dsa_helpers.ml.datasets.utils import create_segformer_segmentation_dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "idx2label = {\n",
    "    1: \"Gray Matter\",\n",
    "    2: \"White Matter\",\n",
    "    3: \"Superficial\",\n",
    "    4: \"Leptomeninges\"\n",
    "}\n",
    "\n",
    "LINE_COLORS = {\n",
    "    1: \"rgb(0,128,0)\",\n",
    "    2: \"rgb(0,0,255)\",\n",
    "    3: \"rgb(255,255,0)\",\n",
    "    4: \"rgb(0,0,0)\"\n",
    "}\n",
    "\n",
    "FILL_COLORS = {\n",
    "    1: \"rgba(0,128,0,0.5)\",\n",
    "    2: \"rgba(0,0,255,0.5)\",\n",
    "    3: \"rgba(255,255,0,0.5)\",\n",
    "    4: \"rgba(0,0,0,0.5)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Functions\n",
    "def compute_metrics(batch_output: tuple[np.ndarray, np.ndarray]) -> dict:\n",
    "    \"\"\"Compute metrics for a batch of predictions from SegFormer model\n",
    "    of multi-class labels.\n",
    "    \n",
    "    Args:\n",
    "        batch_output (tuple[numpy.ndarray, numpy.ndarray]): Tuple of logits and \n",
    "            labels.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = batch_output\n",
    "        \n",
    "        # Convert logits to tensor.\n",
    "        logits_tensor = torch.from_numpy(logits).cpu()\n",
    "        labels_tensor = torch.from_numpy(labels).cpu()\n",
    "        \n",
    "        # From logits get the number of classes in the dataset.\n",
    "        num_classes = logits.shape[1]\n",
    "        \n",
    "        # Scale the logits back to the shape of the labels.\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        \n",
    "        # Turn the logits to predictions.\n",
    "        pred = logits_tensor.argmax(dim=1)\n",
    "            \n",
    "        # Calculate the IoU for each class.\n",
    "        ious = []\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "            pred_mask = (pred == cls)\n",
    "            labels_mask = (labels_tensor == cls)\n",
    "            \n",
    "            intersection = torch.logical_and(pred_mask, labels_mask).sum().item()\n",
    "            union = torch.logical_or(pred_mask, labels_mask).sum().item()\n",
    "            \n",
    "            if union == 0:\n",
    "                iou = float('nan')  # or 1.0\n",
    "            else:\n",
    "                iou = intersection / union\n",
    "                \n",
    "            metrics[f'label {cls} IoU'] = iou\n",
    "                \n",
    "            ious.append(iou)\n",
    "            \n",
    "        mean_iou = np.nanmean(ious)  # Use NumPy for NaN handling\n",
    "        metrics['mean_iou'] = mean_iou\n",
    "            \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = \"/rayCode/bert2024/candanoDev/uk_wsi\"\n",
    "RANDOM_STATE = 42\n",
    "VAL_FRAC = 0.2  # fraction of WSIs to use for validation\n",
    "LABEL_2_IDX = {\n",
    "    \"Background\": 0,\n",
    "    \"Gray Matter\": 1,\n",
    "    'White Matter': 2,\n",
    "    'Superficial': 3,\n",
    "    \"Leptomeninges\": 4\n",
    "}\n",
    "SAVE_DIR = \"/rayCode/bert2024/candanoDev/uk_wsi/experiments\"\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "EVAL_ACCUMULATION_STEPS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(DATASET_DIR)\n",
    "\n",
    "# Metadata for each tile image.\n",
    "tile_df = pd.read_csv(dataset_dir / \"tile_10_metadata.csv\")\n",
    "val_df = pd.read_csv(dataset_dir / \"tile_test10_metadata.csv\")\n",
    "print(f\"{len(tile_df)} tiles found.\")\n",
    "tile_df.head()\n",
    "# Get list of unique source WSI image names.\n",
    "train_wsi_names = tile_df['wsi_name'].unique()\n",
    "val_wsi_names = val_df['wsi_name'].unique()\n",
    "\n",
    "# Split the unique WSI names into training and validation sets.\n",
    "#train_wsi_names, val_wsi_names = train_test_split(wsi_names, test_size=VAL_FRAC, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Training set has {len(train_wsi_names)} WSI images.\")\n",
    "print(f\"Validation set has {len(val_wsi_names)} WSI images.\")\n",
    "# Split the tiles into training and validation sets.\n",
    "train_tiles_df = tile_df[tile_df[\"wsi_name\"].isin(train_wsi_names)]\n",
    "val_tiles_df = tile_df[val_df[\"wsi_name\"].isin(val_wsi_names)]\n",
    "\n",
    "## Remove after testing\n",
    "train_tiles_df = train_tiles_df.head(900)\n",
    "val_tiles_df = val_tiles_df.head(100)\n",
    "\n",
    "print(f\"Training set has {len(train_tiles_df)} tiles.\")\n",
    "print(f\"Validation set has {len(val_tiles_df)} tiles.\")\n",
    "# Create SegFormer dataset objects.\n",
    "train_dataset = create_segformer_segmentation_dataset(  \n",
    "    train_tiles_df, transforms=train_transforms\n",
    ")\n",
    "val_dataset = create_segformer_segmentation_dataset(\n",
    "    val_tiles_df, transforms=val_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ID to label from label to id.\n",
    "id_2_label = {v: k for k, v in LABEL_2_IDX.items()}\n",
    "\n",
    "# Load a model to start training from.\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", id2label=id_2_label, label2id=LABEL_2_IDX\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(SAVE_DIR) / \"model\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    model_dir,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_accumulation_steps=EVAL_ACCUMULATION_STEPS,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "from shapely.affinity import translate\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "# Functions\n",
    "def mask_to_geojson(\n",
    "    mask: str | np.ndarray, x_offset: int = 0, y_offset: int = 0, \n",
    "    background_label: int = 0, min_area: int = 0\n",
    ") -> list[Polygon, int]:\n",
    "    \"\"\"\n",
    "    Extract contours from a label mask and convert them into shapely \n",
    "    polygons.\n",
    "    \n",
    "    Args:\n",
    "        mask (str | np.ndarray): Path to the mask image or the mask.\n",
    "        x_offset (int): Offset to add to x coordinates of polygons.\n",
    "        y_offset (int): Offset to add to y coordinates of polygons.\n",
    "        background_label (int): Label value of the background class, \n",
    "            which is ignored.\n",
    "    \n",
    "    Returns:\n",
    "        list[Polygon, int]: List of polygons and their corresponding \n",
    "            labels.\n",
    "            \n",
    "    \"\"\"\n",
    "    if isinstance(mask, str):\n",
    "        mask = imread(mask, grayscale=True)\n",
    "    \n",
    "    # Find unique labels (excluding background 0)\n",
    "    labels = [label for label in np.unique(mask) if label != background_label]\n",
    "    \n",
    "    polygons = []  # Track all polygons.\n",
    "        \n",
    "    # Loop through unique label index.\n",
    "    for label in labels:\n",
    "        # Filter to mask for this label.\n",
    "        label_mask = (mask == label).astype(np.uint8)\n",
    "        \n",
    "        # Find contours.\n",
    "        contours, hierarchy = cv.findContours(\n",
    "            label_mask, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        \n",
    "        # Process the contours.\n",
    "        polygons_dict = {}\n",
    "\n",
    "        for idx, (contour, h) in enumerate(zip(contours, hierarchy[0])):\n",
    "            if len(contour) > 3:\n",
    "                if idx not in polygons_dict:\n",
    "                    polygons_dict[idx] = {\"holes\": []}\n",
    "                \n",
    "                if h[3] == -1:\n",
    "                    polygons_dict[idx][\"polygon\"] = contour.reshape(-1, 2)\n",
    "                else:\n",
    "                    polygons_dict[h[3]][\"holes\"].append(contour.reshape(-1, 2))\n",
    "        \n",
    "        # Now that we know the polygon and the holes, create a Polygon object for each.\n",
    "        for data in polygons_dict.values():   \n",
    "            if 'polygon' in data: \n",
    "                polygon = Polygon(data[\"polygon\"], holes=data[\"holes\"])\n",
    "                \n",
    "                # Shift the polygon by the offset.\n",
    "                polygon = translate(polygon, xoff=x_offset, yoff=y_offset)\n",
    "                            \n",
    "                # Skip small polygons.\n",
    "                if polygon.area >= min_area:\n",
    "                    polygons.append([polygon, label])\n",
    "        \n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 1\n",
    "# Find the checkpoint directory in the model directory.\n",
    "checkpoint_dir = None\n",
    "\n",
    "# Load the model.\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"experiments/model/checkpoint-174\")\n",
    "# Read the datasets from csv.\n",
    "train_tiles_df = pd.read_csv(\"tile_test10_metadata.csv\")\n",
    "val_tiles_df = pd.read_csv(\"tile_test10_metadata.csv\")\n",
    "\n",
    "# Create SegFormer dataset objects.\n",
    "train_dataset = create_segformer_segmentation_dataset(  \n",
    "    train_tiles_df, transforms=train_transforms\n",
    ")\n",
    "val_dataset = create_segformer_segmentation_dataset(\n",
    "    val_tiles_df, transforms=val_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(val_tiles_df['wsi_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "from dsa_helpers.girder_utils import login\n",
    "\n",
    "gc = login(\"http://bdsa.pathology.emory.edu:8080/api/v1\")\n",
    "itms_list = [i for i in gc.listItem('673b566f900c0c0559bf156f')]\n",
    "\n",
    "\n",
    "# Predict labels on the validation dataset.\n",
    "for wsi_name in list(val_tiles_df['wsi_name'].unique()):\n",
    "    wsi_tiles_df = val_tiles_df[val_tiles_df['wsi_name'] == wsi_name]\n",
    "    # Create dataset for this WSI tiles.\n",
    "    wsi_dataset = create_segformer_segmentation_dataset(\n",
    "        wsi_tiles_df, transforms=val_transforms\n",
    "    )    \n",
    "    # Predict the labels.\n",
    "    predictions = trainer.predict(wsi_dataset).predictions\n",
    "    \n",
    "    pred_tensor = torch.from_numpy(predictions)\n",
    "\n",
    "    # From logits get the number of classes in the dataset.\n",
    "    num_classes = pred_tensor.shape[1]\n",
    "\n",
    "    # Scale the logits back to the shape of the labels.\n",
    "    pred_tensor = nn.functional.interpolate(\n",
    "        pred_tensor,\n",
    "        size=(512, 512),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    # Turn the logits to predictions.\n",
    "    pred_tensor = pred_tensor.argmax(dim=1)\n",
    "    pred_tensor.shape\n",
    "\n",
    "    # Create location to save predictions.\n",
    "    polygons_with_labels = []\n",
    "    tqdm.pandas()\n",
    "    sf = 5 / 40  # hard-coded scaling factor to go from scan magnification to tile magnification\n",
    "\n",
    "    for i in range(len(wsi_tiles_df)):\n",
    "        row = wsi_tiles_df.iloc[i]\n",
    "        pred = pred_tensor[i].numpy()\n",
    "        \n",
    "        # If the tile has no predictions, skip it.\n",
    "        if np.count_nonzero(pred) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Process the mask for polygons.\n",
    "        polygons_with_labels.extend(mask_to_geojson(\n",
    "            pred, x_offset=int(row[\"x\"] ), y_offset=int(row[\"y\"])\n",
    "        ))\n",
    "    \n",
    "    gdf = GeoDataFrame(polygons_with_labels, columns=[\"geometry\", \"label\"])\n",
    "\n",
    "    # Apply a buffer to make edges touch.\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].buffer(1)\n",
    "\n",
    "    gdf.plot(edgecolor=\"black\", column=\"label\")\n",
    "    # Dissolve the dataframe by the label.\n",
    "    gdf_dissolved = gdf.dissolve(by=\"label\", as_index=False)\n",
    "\n",
    "    # Apply the buffer to the dissolved dataframe.\n",
    "    gdf_dissolved[\"geometry\"] = gdf_dissolved[\"geometry\"].buffer(-1)\n",
    "    gdf_dissolved.plot(edgecolor=\"black\", column=\"label\")\n",
    "\n",
    "    for dict_item in itms_list:\n",
    "        if dict_item['name']== wsi_name:\n",
    "            print(dict_item['_id'])\n",
    "            save_id = dict_item['_id']\n",
    "            print(dict_item['name'])\n",
    "\n",
    "    # Format the multipolygons into DSA annotation format.\n",
    "    elements = []\n",
    "    tolerance = 0.5\n",
    "\n",
    "    for idx, row in gdf_dissolved.iterrows():\n",
    "        list_of_polygons = list(row['geometry'].geoms)\n",
    "        label = row['label']\n",
    "        \n",
    "        for poly in list_of_polygons:\n",
    "            exterior_poly = list(poly.exterior.coords)\n",
    "            interior_polys = [list(interior.coords) for interior in poly.interiors]\n",
    "            \n",
    "            if len(exterior_poly) > 1000:\n",
    "                poly = poly.simplify(tolerance, preserve_topology=True)\n",
    "                exterior_poly = list(poly.exterior.coords)\n",
    "            \n",
    "            points = [\n",
    "                [int(xy[0]) * 8, int(xy[1]) * 8, 0] for xy in exterior_poly # *2 for 20 mag\n",
    "            ]\n",
    "            \n",
    "            holes = []\n",
    "            \n",
    "            for interior_poly in interior_polys:\n",
    "                hole = [\n",
    "                    [int(xy[0]) * 8, int(xy[1]) * 8, 0] for xy in interior_poly # *2 for 20 mag\n",
    "                ]\n",
    "                holes.append(hole)\n",
    "\n",
    "            element = {\n",
    "                    \"points\": points,\n",
    "                    \"fillColor\": FILL_COLORS[label],\n",
    "                    \"lineColor\": LINE_COLORS[label],\n",
    "                    \"type\": \"polyline\",\n",
    "                    \"lineWidth\": 2,\n",
    "                    \"closed\": True,\n",
    "                    \"label\": {\"value\": idx2label[label]},\n",
    "                    \"group\": \"Gray White Segmentation\"\n",
    "            }\n",
    "            \n",
    "            if len(holes):\n",
    "                element[\"holes\"] = holes\n",
    "                \n",
    "            elements.append(element)\n",
    "    gc = login(\"http://bdsa.pathology.emory.edu:8080/api/v1\")\n",
    "    _ = gc.post(\n",
    "        f'/annotation?itemId={save_id}', \n",
    "        json={\n",
    "            'name': \"temp\", \n",
    "            'description': '', \n",
    "            'elements': elements\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg-tissue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
